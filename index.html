<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shubham Agrawal</title>

  <meta name="author" content="Shubham Agrawal">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü§ñ</text></svg>">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Shubham Agrawal</name>
                  </p>
                  <p>
                    I am a Senior Researcher at <a href=https://research.samsung.com/aicenter_ny>
                      Samsung Research America</a> in New York City, where I work on robotics,
                    computer vision, and machine learning. I am fortunate to be working under supervision
                    of Prof. <a href="https://www-users.cse.umn.edu/~isler/" target="_blank">Volkan Isler</a>.
                    Before joining Samsung, I received my CS masters from Columbia University with a
                    focus on robotics and computer vision. During that time, I did several research
                    projects under supervision of Prof. <a href="https://shurans.github.io/" target="_blank">
                    Shuran Song</a>. I did my Bachelors in Computer Science from
                    <a href="https://www.iitk.ac.in/" target="_blank">IIT Kanpur</a>.
                    In past, I also worked at <a href="https://www.tesla.com/"
                      target="_blank">Tesla Inc</a>
                    and <a href="https://www.adobe.com/" target="_blank">Adobe Sytems</a> as software development
                    engineer.
                  </p>
                  <!-- <p>
                    At Google I've worked on <a href="https://www.google.com/glass/start/">Glass</a>, <a
                      href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a
                      href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a
                      href="https://blog.google/products/google-ar-vr/introducing-next-generation-jump/">Jump</a>, <a
                      href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait
                      Mode</a>, <a
                      href="https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html">Portrait
                      Light</a>, and <a href="https://www.matthewtancik.com/nerf">NeRF</a>. I did my PhD at <a
                      href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a
                      href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and funded by the <a
                      href="http://www.nsfgrfp.org/">NSF GRFP</a>. I've received the <a
                      href="https://www2.eecs.berkeley.edu/Students/Awards/15/">C.V. Ramamoorthy Distinguished Research
                      Award</a> and the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher
                      Award</a>.
                  </p> -->
                  <p style="text-align:center">
                    <a href="mailto:agshubh191@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="./images/CV_Shubham_Agrawal.pdf">CV</a> &nbsp/&nbsp
                    <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                    <a href="https://scholar.google.com/citations?user=VbfC5GMAAAAJ">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://twitter.com/submagr">Twitter</a> &nbsp/&nbsp
                    <a href="https://github.com/submagr/">Github</a> &nbsp/&nbsp
                    <a href="https://linkedin.com/in/submagr/">LinkedIn</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="./images/ShubhamAgrawal.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="./images/ShubhamAgrawalCircle.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    <!-- I'm interested in computer vision, machine learning, optimization, and image processing. Much of my
                    research is about inferring the physical world (shape, motion, color, light, etc) from images.
                    Representative papers are <span class="highlight">highlighted</span>. -->
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <!-- ================ SceneGrasp ================ -->
              <tr onmouseout="scenegrasp_stop()" onmouseover="scenegrasp_start()">
                <td style="padding:20px;width:40%;vertical-align:middle">
                  <div class="one">
                    <div class="two" >
                      <video width=100% muted autoplay loop>
                        <source src="./images/scenegrasp/teaser.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                    </div>
                  </div>
                </td>

                <td style="padding:20px;width:60%;vertical-align:middle">
                  <a href="https://samsunglabs.github.io/SceneGrasp-project-page/">
                    <papertitle>
                      Real-time Simultaneous Multi-Object 3D Shape Reconstruction, 6DoF Pose
                      Estimation and Dense Grasp Prediction
                    </papertitle>
                  </a>
                  <br>
                  <strong>Shubham Agrawal</strong>,
                  <a href="https://www.linkedin.com/in/nikhilcd">Nikhil Chavan-Dafle</a>,
                  <a href="https://kasai2020.github.io/">Isaac Kasahara</a>,
                  <a href="https://ksengin.github.io/">Selim Engin</a>,
                  <a href="https://sites.google.com/view/jinwookhuh/">Jinwook Huh</a>,
                  <a href="https://www-users.cse.umn.edu/~isler/">Volkan Isler</a>
                  <br>
                  <em>IROS</em>, 2023
                  <br>
                  <a href="https://samsunglabs.github.io/SceneGrasp-project-page/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2305.09510">arXiv</a>
                  /
                  <a href="https://github.com/SamsungLabs/SceneGrasp">code</a>
                  <p></p>
                  <p>
                    We present a novel method to provide geometric and semantic
                    information of all objects in the scene as well as feasible grasps
                    on those objects simultaneously. The main advantage of our
                    method is its speed as it avoids sequential perception and grasp
                    planning steps
                  </p>
                </td>
              </tr>

              <!-- ================ RIC ================ -->
              <tr onmouseout="ric_stop()" onmouseover="ric_start()">
                <td style="padding:20px;width:40%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='ric_video'>
                      <img src='./images/ric/teaser.gif' width="100%" id="ric_video">
                    </div>
                  </div>
                </td>

                <td style="padding:20px;width:60%;vertical-align:middle">
                  <a href="https://samsunglabs.github.io/RIC-project-page/">
                    <papertitle>
                      RIC: Rotate-Inpaint-Complete for Generalizable Scene Reconstruction
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://www.linkedin.com/in/isaac-kasahara">Isaac Kasahara</a>,
                  <strong>Shubham Agrawal</strong>,
                  <a href="https://ksengin.github.io/">Selim Engin</a>,
                  <a href="https://www.linkedin.com/in/nikhilcd">Nikhil Chavan-Dafle</a>,
                  <a href="https://shurans.github.io/">Shuran Song</a>
                  <a href="https://www-users.cse.umn.edu/~isler/">Volkan Isler</a>
                  <br>
                  <em>preprint</em>
                  <br>
                  <a href="https://samsunglabs.github.io/RIC-project-page/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2307.11932">arXiv</a>
                  /
                  <a href="https://github.com/SamsungLabs/RIC">code</a>
                  <p></p>
                  <p>
                    We present a method for scene reconstruction by structurally breaking the
                    problem into two steps: rendering novel views via inpainting and 2D to 3D scene
                    lifting. Specifically, we leverage the generalization capability of large visual
                    language models (Dalle-2) to inpaint the missing areas of scene color images
                    rendered from different views. Next, we lift these inpainted images to 3D by
                    predicting normals of the inpainted image and solving for the missing depth
                    values.
                  </p>
                </td>
              </tr>

              <!-- ================ Shell-Grasp ================ -->
              <tr onmouseout="shellgrasp_stop()" onmouseover="shellgrasp_start()">
                <td style="padding:20px;width:40%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='shellgrasp_video'>
                      <video width=100% muted autoplay loop>
                        <source src="./images/shellgrasp/teaser.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                    </div>
                  </div>
                </td>
                <td style="padding:20px;width:60%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2109.06837.pdf">
                    <papertitle>
                      Simultaneous Object Reconstruction and Grasp Prediction using a Camera-centric
                      Object Shell Representation
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://www.linkedin.com/in/nikhilcd">Nikhil Chavan-Dafle</a>,
                  <a href="https://www.linkedin.com/in/sergiy-popovych">Sergiy Popovych</a>,
                  <strong>Shubham Agrawal</strong>,
                  <a href="https://scholar.google.com/citations?user=J0l7wWwAAAAJ&hl=en">Daniel D. Lee</a>,
                  <a href="https://www-users.cse.umn.edu/~isler/">Volkan Isler</a>
                  <br>
                  <em>IROS, 2022</em>
                  <br>
                  <a href="https://www.youtube.com/watch?v=zbADot4n9fc">
                  video  
                  </a>
                  /
                  <a href="https://arxiv.org/abs/2109.06837">arXiv</a>
                  /
                  <a href="https://github.com/SamsungLabs/ShellRecontruction">code</a>
                  <p></p>
                  <p>
                    We present a new approach to simultaneously reconstruct a mesh and a dense
                    grasp quality map of an object from a depth image. At the core of our approach
                    is a novel cameracentric object representation called the ‚Äúobject shell‚Äù which
                    is composed of an observed ‚Äúentry image‚Äù and a predicted ‚Äúexit image‚Äù.
                  </p>
                </td>
              </tr>

              <!-- ================ SEaT ================ -->
              <tr onmouseout="seat_stop()" onmouseover="seat_start()">
                <td style="padding:20px;width:40%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='seat_video'>
                      <video width=100% height=100% muted autoplay loop>
                        <source src="./images/seat/teaser.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                    </div>
                  </div>
                </td>

                <td style="padding:20px;width:60%;vertical-align:middle">
                  <a href="https://seat.cs.columbia.edu/">
                    <papertitle>
                      Scene Editing as Teleoperation (SEaT): A Case Study in 6DoF Kit Assembly
                    </papertitle>
                  </a>
                  <br>
                  <strong>Shubham Agrawal*</strong>,
                  <a href="https://www.columbia.edu/~yl4095/">Yulong Li*</a>,
                  <a href="https://www.linkedin.com/in/jen-shuo-liu-820945149/">Jen-Shuo Liu</a>,
                  <a href="http://www.cs.columbia.edu/~feiner/">Steven K. Feiner</a>,
                  <a href="https://www.cs.columbia.edu/~shurans/">Shuran Song</a>
                  <br>
                  <em>IROS</em>, 2022
                  <br>
                  <a href="https://seat.cs.columbia.edu/">project page</a>
                  /
                  <a href="https://www.youtube.com/watch?v=F69_2J68Thc">video</a>
                  /
                  <a href="https://arxiv.org/abs/2110.04450">arXiv</a>
                  /
                  <a href="https://github.com/columbia-ai-robotics/SEaT">code</a>
                  <p></p>
                  <p>
                    We create a manipulatable digital-twin of a real-world 6DoF-Kitting scene
                    and propose a deep-learning based approach to infer precise 6DoF object poses
                    inside kit from imprecise poses provided by user.
                  </p>
                </td>
              </tr>

              <!-- ================ Adagrasp ================ -->
              <tr onmouseout="adagrasp_stop()" onmouseover="adagrasp_start()">
                <td style="padding:20px;width:40%;vertical-align:middle">
                  <div class="one">
                    <div class="two">
                      <video width=100% muted autoplay loop id="adagrasp_video">
                        <source src="./images/adagrasp/teaser.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                    </div>
                  </div>
                </td>

                <td style="padding:20px;width:60%;vertical-align:middle">
                  <a href="https://adagrasp.cs.columbia.edu/">
                    <papertitle>
                      AdaGrasp: Learning an Adaptive Gripper-Aware Grasping Policy
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://zhenjiaxu.com/">Zhenjia Xu</a>,
                  <a href="https://www.linkedin.com/in/beichun-qi/">Beichun Qi</a>,
                  <strong>Shubham Agrawal</strong>,
                  <a href="https://www.cs.columbia.edu/~shurans/">Shuran Song</a>
                  <br>
                  <em>ICRA</em>, 2021
                  <br>
                  <a href="https://adagrasp.cs.columbia.edu/">project page</a>
                  /
                  <a href="https://www.youtube.com/watch?v=kknTYTbORfs">video</a>
                  /
                  <a href="https://arxiv.org/abs/2011.14206">arXiv</a>
                  /
                  <a href="https://github.com/columbia-ai-robotics/adagrasp">code</a>
                  <p>
                    We learn a single grasping policy that generalizes to novel grippers.
                  </p>
                </td>
              </tr>

              <!-- ================ Fit2Form ================ -->
              <tr onmouseout="fit2form_stop()" onmouseover="fit2form_start()">
                <td style="padding:20px;width:40%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='fit2form_image'>
                      <video width=100% muted autoplay loop>
                        <source src="./images/fit2form/teaser.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                    </div>
                  </div>
                </td>

                <td style="padding:20px;width:60%;vertical-align:middle">
                  <a href="https://fit2form.cs.columbia.edu/">
                    <papertitle>
                      Fit2Form: 3D Generative Model for Robot Gripper Form Design
                    </papertitle>
                  </a>
                  <br>
                  <strong>Shubham Agrawal*</strong>,
                  <a href="https://www.cs.columbia.edu/~huy/">Huy Ha*</a>,
                  <a href="https://www.cs.columbia.edu/~shurans/">Shuran Song</a>
                  <br>
                  <em>CoRL</em>, 2020
                  <br>
                  <a href="https://fit2form.cs.columbia.edu/">project page</a>
                  /
                  <a href="https://www.youtube.com/watch?v=utKHP3qb1bg">video</a>
                  /
                  <a href="https://arxiv.org/abs/2011.06498">arXiv</a>
                  /
                  <a href="https://github.com/columbia-ai-robotics/fit2form">code</a>
                  <p></p>
                  <p>
                    Given an object to be grasped, we create a 3D generative model to generate 3D
                    geometry of parallel jaw gripper fingers that optimizes various design
                    objectives.
                  </p>
                </td>
              </tr>

            </tbody>
          </table>


          <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>Misc</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>

              <tr>
                <td style="padding:20px;width:40%;vertical-align:middle"><img src="./images/cvf.jpg"></td>
                <td width="75%" valign="center">
                  <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                  <br>
                  <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member,
                    CVPR 2021</a>
                  <br>
                  <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                  <br>
                  <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:40%;vertical-align:middle">
                  <img src="./images/cs188.jpg" alt="cs188">
                </td>
                <td width="75%" valign="center">
                  <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor,
                    CS188 Spring 2011</a>
                  <br>
                  <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor,
                    CS188 Fall 2010</a>
                  <br>
                  <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd
                    Edition</a>
                </td>
              </tr>


              <tr>
                <td align="center" style="padding:20px;width:40%;vertical-align:middle">
                  <heading>Basically <br> Blog Posts</heading>
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                  <br>
                  <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain
                    Functions</a>
                  <br>
                  <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                </td>
              </tr>


            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source
                      code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics
                    tags that you do not want on your own website &mdash; use the github code instead. Also, consider
                    using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a
                      href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table> -->
</body>

</html>